#!/usr/bin/env python3

import re
import nmap
import asyncio
from cmds.scrapyCmd import MySpider
from scrapy.crawler import CrawlerProcess
from cmds.sqlmapCmd import run_sqlmap
from cmds.lfimapCmd import run_lfimap

def scan_target(target):
    # Create an Nmap scanner instance
    nm = nmap.PortScanner()

    websitePorts = {
        "httpPorts": [],
        "httpsPorts": []
    }

    # Run a quick scan on the target host
    nm.scan(target, '20-1024')  # Scanning ports 20 through 1024

    # Print the scan results
    for host in nm.all_hosts():
        if nm[host].state() != 'up':
            print("Host is down")
            return
        # print(f'Scan results for {host}:')
        for proto in nm[host].all_protocols():
            lport = nm[host][proto].keys()
            for port in lport:
                # print(f'Port : {port}, State : {nm[host][proto][port]["state"]}, Service: {nm[host][proto][port]["name"]}')
                portService = nm[host][proto][port]["name"]
                state = nm[host][proto][port]["state"]
                if portService == "http" and state == "open":
                    websitePorts["httpPorts"].append(port)
                elif portService == "https" and state == "open":
                    websitePorts["httpsPorts"].append(port)

    return websitePorts

# This creates the spider used for crawling the site provided
def run_spider(target):
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',
    })

    process.crawl(MySpider, url=f'http://{target}/login/')
    process.start()

    # Access the collected URLs from the spider's collected_urls attribute - also remove duplicates
    collected_urls = list(dict.fromkeys(MySpider.collected_urls))

    return collected_urls

def process_urls(urls):
    login_urls = []
    sql_lfi_urls = []

    all_urls = {}
    
    login_pattern = re.compile(r'https?:\/\/[\w.-]+(?:\.[\w.-]+)+(?:\/[\w\-\._~:/?#\[\]@!$&\'()*+,;=]*)?(\/login|\/login\.php)(\?[\w\-\._~:/?#\[\]@!$&\'()*+,;=]*)?$')
    sql_lfi_pattern = re.compile(r'https?:\/\/[\w.-]+(?:\.[\w.-]+)+[\w\-\._~:/?#\[\]@!$&\'()*+,;=]*(\?|&)([\w.-]+=[\w.-]+)')
    sql_lfi_urls = [url for url in urls if sql_lfi_pattern.search(url)]
    login_urls = [url for url in urls if login_pattern.search(url)]

    all_urls["login"] = login_urls
    all_urls["sql_lfi"] = filter_unique_url_ids(sql_lfi_urls)

    print("URLs Processed") 
    return all_urls

## placeholder
async def test_func():
    print("yeet")

def filter_unique_url_ids(urls):
    unique_urls = {}
    for url in urls:
        # Split the URL at the '?'
        base_url, query_string = url.split('?')
        # Use the combination of base URL and id as the key
        if base_url not in unique_urls:
            unique_urls[base_url] = url

    # Now get the list of unique URLs
    unique_urls_list = list(unique_urls.values())

    return(unique_urls_list)

async def async_main(target):
    collected_urls = run_spider(target)
    all_urls = process_urls(collected_urls)
    #print(all_urls)

    login_urls = all_urls["login"]
    sql_lfi_urls = all_urls["sql_lfi"]

    # These commands run lfimap/sqlmap - Iteration is needed to loop over the array of URLs provided - Just using one for testing however
    sqlmap_task = asyncio.create_task(run_sqlmap(login_urls[0]))
    lfimap_task = asyncio.create_task(run_lfimap(sql_lfi_urls[0]))

    await asyncio.gather(sqlmap_task, lfimap_task)

    sqlmap_output = sqlmap_task.result()
    lfimap_output = lfimap_task.result()
    
    if sqlmap_output is not None:
        if(sqlmap_output[0]):
            print("The login page is vulnerable to SQL Injection!")
            print(sqlmap_output[1])
        else:
            print("This login page is not vulnerable to SQL Injection")
    else:
        print(sqlmap_output[1])
    
    if lfimap_output is not None:
        ## lfimap_output[0] == bool - lfimap_output[1] == vulns found - lfimap_output[2] = URL
        if(lfimap_output[0]):
            print(f"{lfimap_output[2]} is vulnerable to Local File Inclusion!")
            print(lfimap_output[1]) # Number of vulns found
        else:
            print(f"{lfimap_output[2]} is not vulnerable to Local File Inclusion")
    else:
        print(lfimap_output[1])


if __name__ == '__main__':
    target = '192.168.124.132'  # Replace with the IP address of the target host
    asyncio.run(async_main(target))