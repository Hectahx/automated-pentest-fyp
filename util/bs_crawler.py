import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def is_valid_url(url):
    """ Check if the URL is valid """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def get_all_links(url, domain_name):
    """ Return all valid URLs on the specified page within the same domain, including parameters """
    urls = set()
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        for a_tag in soup.findAll("a"):
            href = a_tag.get("href")
            if href is None:
                # href empty tag
                continue
            # Create an absolute URL from a relative URL
            href = urljoin(url, href)
            parsed_href = urlparse(href)
            # Keep URL GET parameters, remove URL fragments
            href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
            if parsed_href.query:
                href += '?' + parsed_href.query
            if not is_valid_url(href) or domain_name not in href:
                # Not a valid URL or it's an external link
                continue
            urls.add(href)
    except requests.exceptions.RequestException as e:
        print(f"Error during requests to {url} : {str(e)}")
    return urls

def crawl(url, max_depth, url_list, visited=set(), current_depth=0):
    """ Recursively crawl starting from the initial URL up to the specified depth """
    if current_depth == 0:
        visited.clear() #As it is a recursive function, visited doesnt clear when running it again so need to clear it
    
    domain_name = urlparse(url).netloc  # Get domain name
    
    if False: # testing
        print(f"The current depth is {current_depth}")
        print(f"The max dept  is {max_depth}")
        print(f"The visited  is {visited}")
        print(f"The URL List  is {url_list}")

    if current_depth == max_depth:
        return
    if url in visited:
        return
    #print(f"Crawling: {url}")
    visited.add(url)
    url_list.append(url)
    links = get_all_links(url, domain_name)
    for link in links:
        crawl(link, max_depth, url_list, visited, current_depth + 1)

if __name__ == '__main__':
    initial_url = "http://192.168.124.132/login/"  # Replace with your starting URL 
    url_list = []
    max_depth = 3  # Replace with desired depth
    crawl(initial_url, max_depth, url_list)

    # The list `all_urls` now contains all the URLs
    print("Collected URLs:")
    for url in url_list:
        print(url)