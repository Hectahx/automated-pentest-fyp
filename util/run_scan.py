from requests.exceptions import RequestException
from util.process_urls import process_urls
from cmds.testsslCmd import run_testssl
from cmds.sqlmapCmd import run_sqlmap
from cmds.lfimapCmd import run_lfimap
from util.csrf_scan import check_csrf
from util.idor_scan import check_idor
from util.bs_crawler import crawl
import requests
import asyncio 

async def run_scan(target):
    collected_urls = []
    max_depth = 3
    #print(target)
    crawl(target, max_depth, collected_urls)
    all_urls = process_urls(collected_urls)

    login_urls = all_urls["login"]
    sql_lfi_urls = all_urls["sql_lfi_idor"]

    #This only scans the first sites of each array, need to change it so all of them are done
    sqlmap_task = asyncio.create_task(run_sqlmap(login_urls[0]))
    lfimap_task = asyncio.create_task(run_lfimap(sql_lfi_urls[0]))
    csrf_task = asyncio.create_task(check_csrf(collected_urls[0]))
    testssl_task = asyncio.create_task(run_testssl(target))

    await asyncio.gather(sqlmap_task, lfimap_task, csrf_task, testssl_task)

    sqlmap_output = sqlmap_task.result()
    lfimap_output = lfimap_task.result()
    csrf_output = csrf_task.result()


    results = {
        "target" : target,
        "sqlmap": {"output": sqlmap_output, "is_vulnerable": sqlmap_output["is_vulnerable"] if sqlmap_output else False},
        "lfimap": {"output": lfimap_output, "is_vulnerable": lfimap_output["is_vulnerable"] if lfimap_output else False},
        "csrf" : {"output" : csrf_output, "is_vulnerable" : csrf_output["possible_csrf_vulnerability"] if csrf_output else False},
        "testssl" : {}
    }

    return results

async def run_scan_multi(target):
    """
    Executes a penetration test on the URL provided. This function uses all URLs crawled
    
    Parameters:
    - results: An array of dictionaries, each containing the result for a single URL.
    
    Returns:
    - A summary dictionary with counts of total, vulnerable, non-vulnerable URLs, errors,
      and a list of vulnerable URLs.
    """
    if not is_website_online(target):
        print("The website is not online")
        summary = {
            "target": target,
            "success": False
        }

        results = {
            "summary" : summary,             
        }

        return results
       


    collected_urls = []
    max_depth = 3
    #print(target)
    if not target.endswith('/'):
        target += '/'
    crawl(target, max_depth, collected_urls)
    all_urls = process_urls(collected_urls)

    login_urls = all_urls["login"]
    sql_lfi_urls = all_urls["sql_lfi_idor"]
    other_urls = all_urls["other"]

    

    # Create tasks for all URLs in login_urls and sql_lfi_urls
    sqlmap_tasks = [asyncio.create_task(run_sqlmap(url)) for url in login_urls + sql_lfi_urls]
    lfimap_tasks = [asyncio.create_task(run_lfimap(url)) for url in sql_lfi_urls]
    csrf_tasks = [asyncio.create_task(check_csrf(url)) for url in login_urls + sql_lfi_urls + other_urls]
    idor_tasks = [asyncio.create_task(check_idor(url)) for url in sql_lfi_urls]

    # Continue with testssl task which is only executed once
    testssl_task = asyncio.create_task(run_testssl(target))

    # Wait for all tasks to complete
    await asyncio.gather(*sqlmap_tasks, *lfimap_tasks, *csrf_tasks, *idor_tasks, testssl_task)
    #await asyncio.gather(*sqlmap_tasks, *lfimap_tasks, *csrf_tasks, *idor_tasks)

    # Collect results
    sqlmap_outputs = [task.result() for task in sqlmap_tasks]
    lfimap_outputs = [task.result() for task in lfimap_tasks]
    csrf_outputs = [task.result() for task in csrf_tasks]
    idor_outputs = [task.result() for task in idor_tasks]

    testssl_output = testssl_task.result()  # Assuming testssl_task returns a result

    #print(collected_urls)

    sqlmap_summary = summarize_results(sqlmap_outputs)
    lfimap_summary = summarize_results(lfimap_outputs)
    csrf_summary = summarize_results(csrf_outputs)
    idor_summary = summarize_results(idor_outputs)
    

    
    summary = {
        "target": target,
        "success": True,
        "sql_injection": sqlmap_summary,
        "lfi": lfimap_summary,
        "csrf": csrf_summary,
        "idor" : idor_summary,
        "testssl": testssl_output
    }

    results = {
        "summary" : summary,
        "output" : {
            "sql_injection": sqlmap_outputs,
            "lfi": lfimap_outputs,
            "csrf": csrf_outputs,
            "idor" : idor_outputs,
            "testssl": testssl_output
        } 
    }

    

    return results

def summarize_results(results):
    """
    Summarizes penetration testing results provided as an array of dictionaries.
    
    Parameters:
    - results: An array of dictionaries, each containing the result for a single URL.
    
    Returns:
    - A summary dictionary with counts of total, vulnerable, non-vulnerable URLs, errors,
      and a list of vulnerable URLs.
    """
    summary = {
        "total": len(results),
        "vulnerable": 0,
        "non_vulnerable": 0,
        "errors": 0,
        "vulnerable_urls": []
    }

    for result in results:
        if result["is_vulnerable"]:
            summary["vulnerable"] += 1
            summary["vulnerable_urls"].append(result["url"])
        else:
            summary["non_vulnerable"] += 1
        if "error" in  result:
            if result["error"]:
                summary["errors"] += 1

    return summary

def is_website_online(url):
    try:
        response = requests.head(url, timeout=5)
        # You might want to check for more status codes depending on your definition of 'online'
        return response.status_code == 200
    except RequestException:
        # Handles all exceptions that might occur during the request
        return False
    


if __name__ == '__main__':
    target = "http://192.168.124.132/login/"  # Replace with your starting URL
    print(target)
    asyncio.run(run_scan(target))