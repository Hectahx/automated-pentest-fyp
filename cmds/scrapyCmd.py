import logging
import scrapy
from scrapy import signals
from scrapy.crawler import CrawlerProcess
from scrapy.spiders import CrawlSpider, Rule
from scrapy.utils.log import configure_logging
from scrapy.signalmanager import dispatcher
from scrapy.linkextractors import LinkExtractor



#configure_logging(install_root_handler=True)
#logging.disable(50)  # CRITICAL = 50

# Disable all logging from Scrapy and its dependencies


class MySpider(CrawlSpider):
    def __init__(self, cookies=None, *args, **kwargs):
        # Extract the URL from kwargs and set it as the start_urls list
        self.start_urls = [kwargs.pop("url")]  # Ensure 'url' is passed as a keyword argument
        #self.collected_urls = []  # Initialize a list to store URLs
        dispatcher.connect(self.closed, signal=signals.spider_closed)#This connects the closed function to when the spider itself closes
        self.cookies = cookies

        super(MySpider, self).__init__(*args, **kwargs)

    name = "myspider"
    # Initialize an empty list to store the URLs
    collected_urls = []  # Initialize a list to store URLs

    #start_urls= ["http://192.168.124.128/login"]

    # Define the rules to follow for crawling links
    rules = (
        Rule(
            LinkExtractor(
                allow=(),  # Allow all links to be extracted
            ),
            callback="parse_item",
            follow=True,  # Follow links (recursively)
        ),
    )

    def parse_item(self, response):
        # Extract and append the URLs to the list
        for link in response.css("a::attr(href)").extract():
            full_url = response.urljoin(link)  # Create a full URL
            self.collected_urls.append(
                full_url
            )  # Append the full URL to the collected_urls list

    # This method is called when the spider is closed.
    def closed(self, reason):
        print("Spider finished")


# This creates the spider used for crawling the site provided
def run_spider(target):
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',
    })

    process.crawl(MySpider, url=f'http://{target}/login/')
    process.start()

    # Access the collected URLs from the spider's collected_urls attribute - also remove duplicates
    collected_urls = list(dict.fromkeys(MySpider.collected_urls))

    return collected_urls

"""
if __name__ == "__main__":

    URL = "http://192.168.124.128/login"

    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',
    })

    # Pass the 'url' keyword argument to the spider
    process.crawl(MySpider, url=URL)
    process.start()


    # Access the collected URLs from the spider's collected_urls attribute - also remove duplicates
    collected_urls = list(dict.fromkeys(MySpider.collected_urls))
    

    for url in collected_urls:
        print(url)

"""
